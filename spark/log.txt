[0m[[0minfo[0m] [0mLoading project definition from /vagrant/scala/akka-scraping/spark/project[0m
[0m[[0minfo[0m] [0mSet current project to spark-kafka (in build file:/vagrant/scala/akka-scraping/spark/)[0m
[0m[[0minfo[0m] [0mCompiling 1 Scala source to /vagrant/scala/akka-scraping/spark/target/scala-2.10/classes...[0m
[0m[[0minfo[0m] [0mRunning org.tycho.scraping.SparkKafka [0m
rdd: KafkaRDD[0] at createDirectStream at SparkKafka.scala:113
[0m[[31merror[0m] [0m(run-main-0) org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost): java.lang.ClassNotFoundException: scala.None$[0m
[0m[[31merror[0m] [0m	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)[0m
[0m[[31merror[0m] [0m	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)[0m
[0m[[31merror[0m] [0m	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)[0m
[0m[[31merror[0m] [0m	at java.lang.Class.forName0(Native Method)[0m
[0m[[31merror[0m] [0m	at java.lang.Class.forName(Class.java:348)[0m
[0m[[31merror[0m] [0m	at org.apache.spark.serializer.JavaDeserializationStream$$anon$1.resolveClass(JavaSerializer.scala:66)[0m
[0m[[31merror[0m] [0m	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1613)[0m
[0m[[31merror[0m] [0m	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1518)[0m
[0m[[31merror[0m] [0m	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1774)[0m
[0m[[31merror[0m] [0m	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)[0m
[0m[[31merror[0m] [0m	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993)[0m
[0m[[31merror[0m] [0m	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918)[0m
[0m[[31merror[0m] [0m	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)[0m
[0m[[31merror[0m] [0m	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)[0m
[0m[[31merror[0m] [0m	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:371)[0m
[0m[[31merror[0m] [0m	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:69)[0m
[0m[[31merror[0m] [0m	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:95)[0m
[0m[[31merror[0m] [0m	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:194)[0m
[0m[[31merror[0m] [0m	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)[0m
[0m[[31merror[0m] [0m	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)[0m
[0m[[31merror[0m] [0m	at java.lang.Thread.run(Thread.java:745)[0m
[0m[[31merror[0m] [0m[0m
[0m[[31merror[0m] [0mDriver stacktrace:[0m
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost): java.lang.ClassNotFoundException: scala.None$
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.spark.serializer.JavaDeserializationStream$$anon$1.resolveClass(JavaSerializer.scala:66)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1613)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1518)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1774)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:371)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:69)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:95)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1266)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1257)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1256)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1256)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1411)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
[0m[[31mtrace[0m] [0mStack trace suppressed: run [34mlast compile:run[0m for the full output.[0m
rdd: KafkaRDD[1] at createDirectStream at SparkKafka.scala:113
rdd: KafkaRDD[2] at createDirectStream at SparkKafka.scala:113
